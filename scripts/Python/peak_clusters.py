import configuration as conf
import Yoshida

import numpy as np
import pandas as pd
import pdb
import master_peaks as mp
import os
from sknetwork.clustering import Louvain
import igraph as ig
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import itertools as it

# Create peak (row) clusters from the binary master idr matrix
class create_peak_clusters: 
    
    peak_cluster_directory = conf.DATA_DIR + "peak_clusters/"
    # file to store edges used in Louvain algorithm
    edge_file = conf.DATA_DIR + "peak_clusters/edge_list.csv"
    # file storing clusters generated by Louvain algorithm
    cluster_file = conf.DATA_DIR + "peak_clusters/clusters.csv"
   
    def __init__(self):
        # load the binary, idr master matrix
        self.m = mp.master_peaks().load_matrix().astype("float")
        if not os.path.isdir(self.peak_cluster_directory):
            os.mkdir(self.peak_cluster_directory)           
        if not os.path.isfile(self.edge_file):
            self.compute_edges()
        if not os.path.isfile(self.cluster_file):
            self.compute_clusters()
        
    def load_edges(self):
        return (pd.read_csv(self.edge_file, sep=","))
        
    def load_clusters(self):
        return (pd.read_csv(self.cluster_file, sep=","))
        
    # computation methods
    def compute_edges_between_blocks(self, n1, n2, distance_cutoff):
   
        m = self.m

        ind1 = (np.where(np.sum(m, 1) == n1))[0]
        mm1 = m[ind1,:]
        ind2 = (np.where(np.sum(m, 1) == n2))[0]
        mm2 = m[ind2,:]
        
        dist_m = np.dot(mm1, np.transpose(1 - mm2))
        np.fill_diagonal(dist_m, np.Inf)

        ce = np.where(dist_m < distance_cutoff)
        if len(ce) == 0:
            return None
       
        edge_d = pd.DataFrame({'e1':ind1[ce[0]], 'e2':ind2[ce[1]]})
        if  n1 == n2:
          edge_d = edge_d.loc[edge_d["e1"] < edge_d["e2"]]
       
        return edge_d
    
    
    def compute_edges(self):
        nc = self.m.shape[1]
        
        d_cutoff = null_peak_clusters().read_cutoff_file()
        d_cutoff = d_cutoff[d_cutoff["rs1"] <= d_cutoff["rs2"]]
        
        edges = []
        for i  in range(len(d_cutoff)):
            cd = d_cutoff.iloc[i]
            ce = self.compute_edges_between_blocks(cd["rs1"], cd["rs2"],
                                                   cd["cutoff"])
            print(["found", len(ce), "edges between blocks",
                   cd["rs1"], cd["rs2"]])
            edges.append(ce)
        
        all_edges = pd.concat(edges)
        all_edges.to_csv(self.edge_file, sep=",", index=False)
        
    
    def louvain(self, edges):
        
        unique_edges = set(edges.iloc[:,0].to_list() + 
                           edges.iloc[:,1].to_list())
        unique_edges = sorted(unique_edges)
        nv = len(unique_edges)
        ud = pd.DataFrame({'e1':unique_edges})
        ud = ud.reset_index(drop=False)
        
        e1_list = edges.merge(ud, how="left")["index"].to_list()
        ud = ud.rename({'e1':'e2'}, axis=1)
        e2_list = edges.merge(ud, how="left")["index"].to_list()
        
        values = np.repeat(1, len(edges))
       
        m = csr_matrix((values, (e1_list, e2_list)), 
                       shape=(nv, nv))
        
        louvain = Louvain(verbose=True)
        labels = louvain.fit_transform(m)
        
        cluster_d = pd.DataFrame({'row':unique_edges,
                          'cluster':labels}).sort_values(["cluster","row"])
        
        return cluster_d
        
    
    def compute_clusters(self, homo_cutoff=2):
        if not os.path.isfile(self.edge_file):
            print["edge file does not exist,  call compute_edges method"]
            return None
        
        edges = self.load_edges()
        cluster_d = self.louvain(edges) 
    
        # get rows that are mostly open
        m = self.m
        homo_open = np.where(np.sum(m, 1) >= (m.shape[1] - homo_cutoff))[0]
        d_home = pd.DataFrame({'row':homo_open,'cluster':-1})
        
        cluster_d.loc[~cluster_d["row"].isin(homo_open)]
        
        joint = d_home.append(cluster_d)
        joint["cluster"] = joint["cluster"] + 1
        joint = joint.sort_values("cluster")
        
        joint.to_csv(self.cluster_file, sep=",", index=False)
         
        return joint
        

class peak_clusters:
    
    def __init__(self):
        # DataFrame with columns row and cluster
        self.clusters = create_peak_clusters().load_clusters()
        self.m = mp.master_peaks().load_matrix()
    
    def load_clusters(self):
        return self.clusters
    
    def load_matrix(self):
        return self.m
    
    def get_cell_types(self):
        return mp.master_peaks().get_cell_types()
    
    def get_matrix_column_names(self):
        return mp.master_peaks().get_cell_types()
    
             
    def get_cluster_info(self, num_clusters):
        info = []
        for i in range(num_clusters):
            m = self.form_cluster_matrix(i)
            g = self.form_cluster_genomics(i)
            
            ci = {'cluster':i,
                  'size':m.shape[0],
                  'promoters':np.mean(g["dTSS"] < 500),
                  'enhancers':np.mean(g["dTSS"] > 3000),
                  'fraction_on':np.mean(m)}
            info.append(ci)
        
        return pd.DataFrame(info)
       
    
    def form_cluster_matrix(self, index):
        
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
      
        m = self.m[rows,:]
        return m
    
    def form_cluster_sequences(self, index):
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
        
        all_seqs = mp.master_peaks().load_sequences()
        seqs = [all_seqs[i] for i in rows]
        
        return seqs
    
    def form_cluster_genomics(self, index):
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
        
        g = mp.master_peaks().load_genomic_information()
        g = g.iloc[rows]
        
        return g
        
    
    # plot column means
    def scatterplot(self, index, axis, permute=False):
        
         m = self.form_cluster_matrix(index)
         pdb.set_trace()
          
         # permute entries within columns (cell_types)
         if permute:
             nperm = m.shape[0]    
             for i in range(m.shape[1]):
               m[:,i] = m[np.random.permutation(nperm),i]
              
         
         if axis == 0:
           vals = np.mean(m, axis=axis)
           plt.ylim(0, 1)
           plt.scatter(range(len(vals)), vals)
           
           return vals
         else:
          vc = pd.DataFrame(np.sum(m, axis=axis)).value_counts()
          index = [x[0] for x in vc.index]
          vals = vc.to_list()
          plt.xlim(0, m.shape[1]+1)
          plt.ylim(0, 1)
          plt.scatter(index, vals)
          
          return pd.DataFrame({'nopen':index, 'count':vals})
        
         
         
    
    def treeplot(self, index):
        m = self.form_cluster_matrix(index)
        m_cell_types = self.get_cell_types()
        nct = len(m_cell_types)
        
        g = Yoshida.Yoshida_tree().load_igraph()
        
        mm = np.mean(m, axis=0)
        val_d = {m_cell_types[i]: mm[i] for i in range(nct)}
        
        vals = np.array([val_d[ct] for ct in g.vs["name"]])
        plt.ylim(0, 1)
        plt.scatter(range(len(vals)), vals)
        
        vs = {}
        vs["bbox"] = (1200, 1000)
        vs["vertex_size"] = 30*vals
        vs["vertex_label_size"] = 20
        vs["vertex_label"] = [str(i)  for i in range(g.vcount())]
        vs["vertex_label_dist"] = 1.5
           
        layout = g.layout_reingold_tilford(mode="all")
     
        pl = ig.plot(g, layout=layout, **vs)
        pl.show()
            
            
    
class null_peak_clusters:
    peak_cluster_directory = conf.DATA_DIR + "peak_clusters/"
    cutoffs_file = peak_cluster_directory + "cutoffs.csv"
    
    def __init__(self):
        self.m = mp.master_peaks().load_matrix()
        if not os.path.isdir(self.peak_cluster_directory):
            os.mkdir(self.peak_cluster_directory)           
        if not os.path.isfile(self.cutoffs_file):
            print("creating cutoffs file...")
            self.create_all_cutoffs()
        
    
    def compute_cutoff(self, rs1, rs2, mc_samples=100):
        m = self.m
        nc = m.shape[1]
        N1 = sum(np.sum(m, 1)==rs1)
        N2 = sum(np.sum(m, 1)==rs2)
        
        print(["clusters contain", N1, N2])
      
        v1 = np.concatenate([np.repeat(1, rs1),
                            np.repeat(0, nc-rs1)])
        v1 = v1.astype("float")
        
        v2 = np.concatenate([np.repeat(1, rs2),
                            np.repeat(0, nc-rs2)])
        v2 = v2.astype("float")
        
        # create a null count matrix
        min_dist = []
        for i in range(mc_samples):
            null_list1 = []
            null_list2 = []
            for i in range(N1):
                null_list1.append(np.random.permutation(v1))
            for i in range(N2):
                null_list2.append(np.random.permutation(v2))
            
            m_null1 = np.array(null_list1)
            m_null2 = np.array(null_list2)
            
            dist_m = np.dot(m_null1, np.transpose(1 - m_null2))
            np.fill_diagonal(dist_m, np.Inf)
      
            min_dist.append(np.min(dist_m))
        
        return min(min_dist)

    def create_all_cutoffs(self, max_delta=5, mc_samples=100):
        nc = self.m.shape[1]
        
        cutoffs = []
        rs1_vals = []
        rs2_vals = []
        
        min_rs = 2
        max_rs = nc - 2
        
        
        rs_vals = np.arange(min_rs, max_rs)
        for rs1 in rs_vals:
          rs2_start = max(min_rs, rs1-max_delta)
          rs2_end = min(max_rs, rs1+max_delta)
          for rs2 in np.arange(rs2_start, rs2_end+1):
            print(["processing row sums equal to", rs1, rs2])
            cutoffs.append(self.compute_cutoff(rs1, rs2, mc_samples))
            rs1_vals.append(rs1)
            rs2_vals.append(rs2)
            
        
        d = pd.DataFrame({'rs1':rs1_vals,
                          'rs2':rs2_vals,
                          'cutoff':cutoffs})
        d.to_csv(self.cutoffs_file, index=False, sep=",")
        
    def read_cutoff_file(self):
        d = pd.read_csv(self.cutoffs_file, sep=",")
        return d
        
        
        