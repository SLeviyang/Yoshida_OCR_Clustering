import configuration as conf

import numpy as np
import pandas as pd
import pdb
import master_peaks as mp
import os
from sknetwork.clustering import Louvain
import igraph as ig
from scipy.sparse import csr_matrix
import matplotlib.pyplot as plt
import itertools as it

# Create peak (row) clusters from the binary master idr matrix
class create_peak_clusters: 
    # load the binary, idr master matrix
    m = mp.master_peaks().load_matrix().astype("float")
    
    peak_cluster_directory = conf.DATA_DIR + "peak_clusters/"
    # file to store edges used in Louvain algorithm
    edge_file = conf.DATA_DIR + "peak_clusters/edge_list.csv"
    # file storing clusters generated by Louvain algorithm
    cluster_file = conf.DATA_DIR + "peak_clusters/clusters.csv"
   
    def __init__(self):
        if not os.path.isdir(self.peak_cluster_directory):
            os.mkdir(self.peak_cluster_directory)           
        if not os.path.isfile(self.edge_file):
            self.compute_edges()
        if not os.path.isfile(self.cluster_file):
            self.compute_clusters()
        
    def load_edges(self):
        return (pd.read_csv(self.edge_file, sep=","))
        
    def load_clusters(self):
        return (pd.read_csv(self.cluster_file, sep=","))
        
    # computation methods
    def compute_edges_between_blocks(self, n1, n2, distance_cutoff):
   
        m = self.m

        ind1 = (np.where(np.sum(m, 1) == n1))[0]
        mm1 = m[ind1,:]
        ind2 = (np.where(np.sum(m, 1) == n2))[0]
        mm2 = m[ind2,:]
        
        dist_m = np.dot(mm1, np.transpose(1 - mm2))
        np.fill_diagonal(dist_m, np.Inf)

        ce = np.where(dist_m < distance_cutoff)
        if len(ce) == 0:
            return None
       
        edge_d = pd.DataFrame({'e1':ind1[ce[0]], 'e2':ind2[ce[1]]})
        if  n1 == n2:
          edge_d = edge_d.loc[edge_d["e1"] < edge_d["e2"]]
       
        return edge_d
    
    
    def compute_edges(self):
        nc = self.m.shape[1]
        
        d_cutoff = null_peak_clusters().read_cutoff_file()
        d_cutoff = d_cutoff[d_cutoff["rs1"] <= d_cutoff["rs2"]]
        
        edges = []
        for i  in range(len(d_cutoff)):
            cd = d_cutoff.iloc[i]
            ce = self.compute_edges_between_blocks(cd["rs1"], cd["rs2"],
                                                   cd["cutoff"])
            print(["found", len(ce), "edges between blocks",
                   cd["rs1"], cd["rs2"]])
            edges.append(ce)
        
        all_edges = pd.concat(edges)
        all_edges.to_csv(self.edge_file, sep=",", index=False)
        
    
    def louvain(self, edges):
        
        unique_edges = set(edges.iloc[:,0].to_list() + 
                           edges.iloc[:,1].to_list())
        unique_edges = sorted(unique_edges)
        nv = len(unique_edges)
        ud = pd.DataFrame({'e1':unique_edges})
        ud = ud.reset_index(drop=False)
        
        e1_list = edges.merge(ud, how="left")["index"].to_list()
        ud = ud.rename({'e1':'e2'}, axis=1)
        e2_list = edges.merge(ud, how="left")["index"].to_list()
        
        values = np.repeat(1, len(edges))
       
        m = csr_matrix((values, (e1_list, e2_list)), 
                       shape=(nv, nv))
        
        louvain = Louvain(verbose=True)
        labels = louvain.fit_transform(m)
        
        cluster_d = pd.DataFrame({'row':unique_edges,
                          'cluster':labels}).sort_values(["cluster","row"])
        
        return cluster_d
        
    
    def compute_clusters(self, homo_cutoff=2):
        if not os.path.isfile(self.edge_file):
            print["edge file does not exist,  call compute_edges method"]
            return None
        
        edges = self.load_edges()
        cluster_d = self.louvain(edges) 
    
        # get rows that are mostly open
        m = self.m
        homo_open = np.where(np.sum(m, 1) >= (m.shape[1] - homo_cutoff))[0]
        d_home = pd.DataFrame({'row':homo_open,'cluster':-1})
        
        cluster_d.loc[~cluster_d["row"].isin(homo_open)]
        
        joint = d_home.append(cluster_d)
        joint["cluster"] = joint["cluster"] + 1
        joint = joint.sort_values("cluster")
        
        joint.to_csv(self.cluster_file, sep=",", index=False)
         
        return joint
        

class peak_clusters:
    
    def __init__(self):
        # DataFrame with columns row and cluster
        self.clusters = create_peak_clusters().load_clusters()
        self.m = mp.master_peaks().load_matrix()
        self.bed = mp.master_peaks().load_table()
    
    def load_bed(self):
        return self.bed
    
    def load_clusters(self):
        return self.clusters
    
    def load_matrix(self):
        return self.m
    
    def get_matrix_column_names(self):
        return mp.master_peaks().get_cell_types()
    
             
    def get_cluster_sizes(self, min_cluster_size=50):
       
        cluster_info = self.clusters["cluster"].value_counts()
        cluster_info = cluster_info[cluster_info > min_cluster_size]
        
        cluster_number = cluster_info.index
        cluster_size = cluster_info.to_list()
        df = pd.DataFrame({'cluster':cluster_number,
                          'size':cluster_size})
        
        return df
    
    def get_cluster_bed(self, index):
        
        bed = self.load_bed()
        clusters = self.load_clusters()
        
        c_rows = clusters.loc[clusters["cluster"]==index]["row"].to_numpy()
        c_bed = bed.iloc[c_rows]
        
        return c_bed
    
    
    def get_cluster_sequence_information(self, min_cluster_size=50):
        
        os.environ["btools"] = conf.BEDTOOLS_PATH
        
        bed = self.load_bed()
        clusters = self.load_clusters()
        t = self.get_cluster_sizes(min_cluster_size)
        uclusters = t["cluster"].unique()
        uclusters.sort()
        
        number_base_pair = []
    
        for c in uclusters:
            c_rows = clusters.loc[clusters["cluster"]==c]["row"].to_numpy()
            c_bed = bed.iloc[c_rows]
            
            # don't need this since idr peaks are 500 bp with no overlap...
            cov = np.sum(c_bed["chrEnd"].to_numpy() - c_bed["chrStart"].to_numpy() + 1)
            number_base_pair.append(cov)
            
        out_t = pd.DataFrame({'cluster':uclusters,
                              'bp':number_base_pair})
        return out_t
    
    def form_cluster_matrix(self, index):
        
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
        
        m = self.m[rows,:]
        return m
    
    def form_cluster_sequences(self, index):
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
        
        all_seqs = mp.master_peaks().load_sequences()
        seqs = [all_seqs[i] for i in rows]
        
        return seqs
            
            
    
class null_peak_clusters:
    cutoffs_file = conf.DATA_DIR + "peak_clusters/cutoffs.csv"
    m = mp.master_peaks().load_matrix()
    
    def __init__(self):
        if not os.path.isfile(self.cutoffs_file):
            print("creating cutoffs file...")
            self.create_all_cutoffs()
        
    
    def compute_cutoff(self, rs1, rs2, mc_samples=100):
        m = self.m
        nc = m.shape[1]
        N1 = sum(np.sum(m, 1)==rs1)
        N2 = sum(np.sum(m, 1)==rs2)
        
        print(["clusters contain", N1, N2])
      
        v1 = np.concatenate([np.repeat(1, rs1),
                            np.repeat(0, nc-rs1)])
        v1 = v1.astype("float")
        
        v2 = np.concatenate([np.repeat(1, rs2),
                            np.repeat(0, nc-rs2)])
        v2 = v2.astype("float")
        
        # create a null count matrix
        min_dist = []
        for i in range(mc_samples):
            null_list1 = []
            null_list2 = []
            for i in range(N1):
                null_list1.append(np.random.permutation(v1))
            for i in range(N2):
                null_list2.append(np.random.permutation(v2))
            
            m_null1 = np.array(null_list1)
            m_null2 = np.array(null_list2)
            
            dist_m = np.dot(m_null1, np.transpose(1 - m_null2))
            np.fill_diagonal(dist_m, np.Inf)
      
            min_dist.append(np.min(dist_m))
        
        return min(min_dist)

    def create_all_cutoffs(self, max_delta=5, mc_samples=100):
        nc = self.m.shape[1]
        
        cutoffs = []
        rs1_vals = []
        rs2_vals = []
        
        min_rs = 2
        max_rs = nc - 2
        
        rs_vals = np.arange(min_rs, max_rs)
        for rs1 in rs_vals:
          rs2_start = max(min_rs, rs1-max_delta)
          rs2_end = min(max_rs, rs1+max_delta)
          for rs2 in np.arange(rs2_start, rs2_end+1):
            print(["processing row sums equal to", rs1, rs2])
            cutoffs.append(self.compute_cutoff(rs1, rs2, mc_samples))
            rs1_vals.append(rs1)
            rs2_vals.append(rs2)
            
        
        d = pd.DataFrame({'rs1':rs1_vals,
                          'rs2':rs2_vals,
                          'cutoff':cutoffs})
        d.to_csv(self.cutoffs_file, index=False, sep=",")
        
    def read_cutoff_file(self):
        d = pd.read_csv(self.cutoffs_file, sep=",")
        return d
        
        
        