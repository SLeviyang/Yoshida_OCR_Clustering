import configuration as conf
import Yoshida
import utilities

import numpy as np
import pandas as pd
import pdb
import master_peaks as mp
import os
from sknetwork.clustering import Louvain
import igraph as ig
from scipy.sparse import csr_matrix
from scipy.stats import hypergeom
import matplotlib.pyplot as plt
import itertools as it
import seaborn as sns

# Create peak (row) clusters from the binary master idr matrix
class create_peak_clusters: 
    
    peak_cluster_directory = conf.DATA_DIR + "peak_clusters/"
    # file to store edges used in Louvain algorithm
    edge_file_prefix = conf.DATA_DIR + "peak_clusters/edge_list_"
    # file storing clusters generated by Louvain algorithm
    cluster_file_prefix = conf.DATA_DIR + "peak_clusters/clusters_"
   
    def __init__(self, FDR, create_clustering=True):
        self.np = null_peak_clusters()
        # load the binary, idr master matrix
        self.m = mp.master_peaks().load_matrix().astype("float")
        
        self.edge_file = self.edge_file_prefix + str(-1*np.log10(FDR)) + ".csv"
        self.cluster_file = self.cluster_file_prefix + str(-1*np.log10(FDR)) + ".csv"
        self.FDR = FDR
        
        if create_clustering:
          if not os.path.isdir(self.peak_cluster_directory):
            os.mkdir(self.peak_cluster_directory)           
          if not os.path.isfile(self.edge_file):
            self.compute_edges()
          if not os.path.isfile(self.cluster_file):
            self.compute_clusters()
        
    def load_edges(self):
        return (pd.read_csv(self.edge_file, sep=","))
        
    def load_clusters(self):
        return (pd.read_csv(self.cluster_file, sep=","))
        
    # computation methods
    def compute_edges_between_blocks(self, n1, n2):
        
        print(["starting computation for edges", n1, n2])
  
        FDR = self.FDR
        m = self.m
        nc = m.shape[1]
        
        ind1 = (np.where(np.sum(m, 1) == n1))[0]
        mm1 = m[ind1,:]
        N1 = len(mm1)
        
        ind2 = (np.where(np.sum(m, 1) == n2))[0]
        mm2 = m[ind2,:]
        N2 = len(mm2)
        
        dist_m = np.dot(mm1, np.transpose(1 - mm2)) \
                 + np.dot(1-mm1, np.transpose(mm2))
        if n1 == n2:
          np.fill_diagonal(dist_m, nc)
    
        # get the sampled distance counts 
        dc = np.zeros([N1, nc+1])
        for k in range(N1):
          cc = np.unique(dist_m[k,:], return_counts=True)
          ind = np.int32(cc[0])
          dc[k,ind] = dc[k,ind] + cc[1]
           
        null_dist_pmf = null_peak_clusters().load_distance_distribution_matrix(n2)[n1,:]
        null_dc = N2*null_dist_pmf
        null_cumsum = np.cumsum(null_dc)

        cutoffs = np.zeros(N1)
        print("identifying significant edges...")
        for i in range(N1):
            sample_cumsum = np.cumsum(dc[i,:])
            pass_FDR = np.where((null_cumsum <= sample_cumsum*FDR) &
                                (sample_cumsum > 0))[0]
            if len(pass_FDR) == 0:
                cutoffs[i] = -1
            else:
                cutoffs[i] = np.max(pass_FDR)
                
        e1 = []
        e2 = []
        print("gathering significant edges...")
        for i in range(N1):
            ce2 = np.where(dist_m[i,:] <= cutoffs[i])[0]
            if not len(ce2) == 0:
                e1 = e1 + np.repeat(i, len(ce2)).tolist()
                e2 = e2 + ce2.tolist()
                
        print(["found", len(e1), "edges between blocks",
                            n1, n2])  
            
       
        
        if len(e1) == 0:
            return None
            
        edge_d = pd.DataFrame({'e1':ind1[e1], 'e2':ind2[e2]})
        if  n1 == n2:
           edge_d = edge_d.loc[edge_d["e1"] < edge_d["e2"]]
       
        return edge_d
    
    
    def compute_edges(self):
        nc = self.m.shape[1]
        
        #d_cutoff = null_peak_clusters().read_cutoff_file()
        #d_cutoff = d_cutoff[d_cutoff["rs1"] <= d_cutoff["rs2"]]
        
        edges = []
        for n1 in range(3,nc-3):
            for delta in [0,1]:
                if n1+delta <= nc-2:
                  n2 = n1 + delta
                  ce = self.compute_edges_between_blocks(n1, n2)
                  if not ce is None:
                     print(["found", len(ce), "edges between blocks",
                            n1, n2])
                     edges.append(ce)
                  else:
                     print(["found", 0, "edges between blocks",
                            n1, n2]) 
        
        all_edges = pd.concat(edges)
        all_edges.to_csv(self.edge_file, sep=",", index=False)
        
    
    def louvain(self, edges):
        
        unique_edges = set(edges.iloc[:,0].to_list() + 
                           edges.iloc[:,1].to_list())
        unique_edges = sorted(unique_edges)
        nv = len(unique_edges)
        ud = pd.DataFrame({'e1':unique_edges})
        ud = ud.reset_index(drop=False)
        
        e1_list = edges.merge(ud, how="left")["index"].to_list()
        ud = ud.rename({'e1':'e2'}, axis=1)
        e2_list = edges.merge(ud, how="left")["index"].to_list()
        
        values = np.repeat(1, len(edges))
       
        m = csr_matrix((values, (e1_list, e2_list)), 
                       shape=(nv, nv))
        
        louvain = Louvain(verbose=True)
        labels = louvain.fit_transform(m)
        
        cluster_d = pd.DataFrame({'row':unique_edges,
                          'cluster':labels}).sort_values(["cluster","row"])
        
        return cluster_d
        
    
    def compute_clusters(self, homo_cutoff=2):
        if not os.path.isfile(self.edge_file):
            print["edge file does not exist,  call compute_edges method"]
            return None
        
        edges = self.load_edges()
        cluster_d = self.louvain(edges) 
    
        # get rows that are mostly open
        m = self.m
        homo_open = np.where(np.sum(m, 1) >= (m.shape[1] - homo_cutoff))[0]
        d_home = pd.DataFrame({'row':homo_open,'cluster':-1})
        
        cluster_d.loc[~cluster_d["row"].isin(homo_open)]
        
        joint = d_home.append(cluster_d)
        joint["cluster"] = joint["cluster"] + 1
        joint = joint.sort_values("cluster")
        
        joint.to_csv(self.cluster_file, sep=",", index=False)
         
        return joint
        

class peak_clusters:
    
    def __init__(self, FDR):
        # DataFrame with columns row and cluster
        self.clusters = create_peak_clusters(FDR).load_clusters()
        self.m = mp.master_peaks().load_matrix()
    
    def load_clusters(self, add_row_sum=False):
        if add_row_sum:
            mm = self.m[self.clusters["row"],:]
            tb = self.clusters.copy()
            tb.insert(2, "count", np.sum(mm,1))
            return tb
        else:
          return self.clusters
    
    def load_matrix(self, restrict_to_clusters=False):
        if restrict_to_clusters:
            return self.m[self.load_clusters()["row"],:]
        else:
          return self.m
    
    def get_cell_types(self):
        return mp.master_peaks().get_cell_types()
    
    def get_matrix_column_names(self):
        return mp.master_peaks().get_cell_types()
    
             
    def get_cluster_info(self, num_clusters=None):
        if num_clusters is None:
            num_clusters = np.max(self.clusters["cluster"]) + 1
        info = []
        for i in range(num_clusters):
            m = self.form_cluster_matrix(i)
            g = self.form_cluster_genomics(i)
            
            ci = {'cluster':i,
                  'size':m.shape[0],
                  'promoters':np.mean(g["dTSS"] < 500),
                  'enhancers':np.mean(g["dTSS"] > 3000),
                  'fraction_on':np.mean(m)}
            info.append(ci)
        
        return pd.DataFrame(info)
       
    
    def form_cluster_matrix(self, index):
        
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
      
        m = self.m[rows,:]
        return m
    
    def form_cluster_sequences(self, index):
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
        
        all_seqs = mp.master_peaks().load_sequences()
        seqs = [all_seqs[i] for i in rows]
        
        return seqs
    
    def form_cluster_genomics(self, index):
        clusters = self.load_clusters()
        rows = clusters.loc[clusters["cluster"]==index]["row"]
        
        g = mp.master_peaks().load_genomic_information()
        g = g.iloc[rows]
        
        return g
        
    
    # plot column means
    def scatterplot(self, index):
        
         m = self.form_cluster_matrix(index)
         m_cell_types = self.get_matrix_column_names()
         y_cell_types = Yoshida.Yoshida_tree().load_igraph().vs["name"]
        
         m = m[:,utilities.match(y_cell_types, m_cell_types)]
          
      
         vc = np.mean(m, 0)
         plt.xlim(0, m.shape[1]+1)
         plt.ylim(0, 1)
         plt.title("Cluster" + str(index))

         plt.scatter(range(len(vc)), vc)
         
        
         
         
    
    def treeplot(self, index, m=None):
        if m is None:
          m = self.form_cluster_matrix(index)
        m_cell_types = self.get_cell_types()
        nct = len(m_cell_types)
        
        g = Yoshida.Yoshida_tree().load_igraph()
        
        mm = np.mean(m, axis=0)
        val_d = {m_cell_types[i]: mm[i] for i in range(nct)}
        
        vals = np.array([val_d[ct] for ct in g.vs["name"]])
        #plt.ylim(0, 1)
        #plt.scatter(range(len(vals)), vals)
        
        vs = {}
        vs["bbox"] = (1200, 1000)
        vs["vertex_size"] = 30*vals
        vs["vertex_label_size"] = 20
        vs["vertex_label"] = [str(i)  for i in range(g.vcount())]
        vs["vertex_label_dist"] = 1.5
           
        layout = g.layout_reingold_tilford(mode="all")
     
        pl = ig.plot(g, layout=layout, **vs)
        pl.show()
        
    def heatmap(self, ncluster=None, min_cluster_size=40):
        if ncluster is None:
          ncluster = np.max(self.clusters["cluster"]) + 1
        means = []
        indices = []
        for index in range(ncluster):
            cm = self.form_cluster_matrix(index)
            if len(cm) > min_cluster_size:
              cm_mean = np.mean(cm, 0)
              means.append(cm_mean.tolist())
              indices.append(index)
        
        means_m = np.array(means)
        m_cell_types = self.get_matrix_column_names()
        y_cell_types = Yoshida.Yoshida_tree().load_igraph().vs["name"]
        
        means_m = means_m[:,utilities.match(y_cell_types, m_cell_types)]
        tb = pd.DataFrame(means_m)
        tb.index = indices
        sns.heatmap(tb)
      
class peak_clusters_comparison:

    def __init__(self, FDR1, FDR2):
        self.pcA = peak_clusters(FDR1)
        self.pcB = peak_clusters(FDR2)

    def heatmap(self, ncluster1, ncluster2, reverse=False):
        if reverse:
          pc1 = self.pcB
          pc2 = self.pcA
        else:
          pc1 = self.pcA
          pc2 = self.pcB
          
        cl1 = pc1.load_clusters()
        cl2 = pc2.load_clusters()
        
        cl1 = cl1[cl1["cluster"] < ncluster1]
        cl2 = cl2[cl2["cluster"] < ncluster2]
        
        overlap_m = np.zeros([ncluster1, ncluster2])
        gcl1 = cl1.groupby("cluster")
        gcl2 = cl2.groupby("cluster")
        for cluster1, group1 in gcl1:
            row1 = set(group1["row"])
            for cluster2, group2 in gcl2:
                row2 = set(group2["row"])
                frac = len(row1.intersection(row2))/len(row1)
                overlap_m[cluster1,cluster2] = frac
                
        sns.heatmap(overlap_m)
        return overlap_m
            
        
        
          
        
            
            
    
class null_peak_clusters:
    peak_cluster_directory = conf.DATA_DIR + "peak_clusters/"
    null_peak_cluster_directory = peak_cluster_directory + "null/"
   
    def __init__(self):
        self.m = mp.master_peaks().load_matrix()
        if not os.path.isdir(self.peak_cluster_directory):
            os.mkdir(self.peak_cluster_directory)   
        if not os.path.isdir(self.null_peak_cluster_directory):
            os.mkdir(self.null_peak_cluster_directory)   
            self.create_distance_distribution_matrices()
        
    # distribution of hamming distance between two rows with n, N 1's,
    # respectively, computed for a fixed nY over all possible nX.
        
    # hypergeometric(M, n, N) gives number of 1's that agree
    # notation follows python hypergeom parameters
    # https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.hypergeom.html
    # M = number of cell types
    # n = 2,...,(number of cell types - 2)
    # N = passed in
    # k = number of 1 overlaps
    # hamming distance = (nX - k) + (nY-k)
    def compute_distance_distribution(self, M, n, N):
        allk = range(M+1)
        pmf = hypergeom.pmf(allk, M, n, N)
        
        dist = np.zeros(M+1)
        for k in range(np.min([M, n+N])):
            hamming_dist = np.int32((n-k) + (N-k))
            if hamming_dist > M:
                hamming_dist = M
            if hamming_dist < 0:
                hamming_dist = 0
            dist[hamming_dist] = dist[hamming_dist] + pmf[k]
            
        return dist
    
    # compute distance distribution for n=0,1,..,number of cell types
    def compute_distance_distribution_matrix(self, N):
        M = self.m.shape[1]
        distm = np.zeros([M,M+1])
        for n in range(M):
            distm[n,:] = self.compute_distance_distribution(M, n, N)
            
        return distm
            
    def create_distance_distribution_matrices(self):
        for N in range(self.m.shape[1]+1):
            print(["creating distance matrix", N])
            d = self.compute_distance_distribution_matrix(N)
          
            cls = ["dist" + str(i) for i in range(d.shape[1])]
            tb = pd.DataFrame(d, columns=cls)
            
            outf = self.null_peak_cluster_directory \
                        + "distance_distribution_" + str(N) + ".csv"
            tb.to_csv(outf, sep=",", index=False)
            
    def load_distance_distribution_matrix(self, N):
         outf = self.null_peak_cluster_directory \
                        + "distance_distribution_" + str(N) + ".csv"
         return pd.read_csv(outf, sep=",").to_numpy()
    
            
    
    
    
        
    
    # def compute_distance_counts(self, rs1, rs2, mc_samples=10):
    #     m = self.m
    #     marginals = np.sum(m, 0)/np.sum(m)
        
    #     nc = m.shape[1]
    #     N1 = np.sum(np.sum(m, 1)==rs1)
    #     N2 = np.sum(np.sum(m, 1)==rs2)
        
    #     m1 = m[np.sum(m, 1)==rs1,:]
    #     m1_indices = np.where(np.sum(m,1)==rs1)[0]
    
    #     print(["clusters contain", N1, N2])
    #     v2 = np.concatenate([np.repeat(1, rs2),
    #                         np.repeat(0, nc-rs2)])
    #     v2 = v2.astype("float")
        
    #     # create null count matrix
    #     # row k contains distance counts to all rows in N2 matrix
    #     distance_counts = np.zeros([N1, nc+1])
    #     m_null2 = np.zeros([N2, nc])
     
    #     for i in range(mc_samples):    
    #         print(["monte carlo", i, "of", mc_samples])
            
    #         m_null2[:,:] = 0
    #         for j in range(N2):
    #             ind = np.random.choice(nc, size=rs2, 
    #                                    replace=False, p=marginals)
    #             m_null2[j,ind] = 1
       
    #         dist_m = np.dot(m1, np.transpose(1 - m_null2)) \
    #                 + np.dot(1 - m1, np.transpose(m_null2))
    #         for k in range(N1):
    #             cc = np.unique(dist_m[k,:], return_counts=True)
    #             ind = np.int32(cc[0])
    #             distance_counts[k,ind] = distance_counts[k,ind] + cc[1]
        
    #     distance_counts = distance_counts/mc_samples
    #     return distance_counts
    #     tb = pd.DataFrame(distance_counts,
    #                       columns=["dist" + str(i) for i in range(nc+1)])
    #     tb.insert(0, "row", m1_indices)
    #     return tb

    # def create_all_cutoffs(self, max_delta=5, mc_samples=100):
    #     nc = self.m.shape[1]
        
    #     distance_m = []
    #     rs1_vals = []
    #     rs2_vals = []
        
    #     min_rs = 2
    #     max_rs = nc - 2
        
    #     #debug 
    #     min_rs = 20
    #     max_rs = 27
        
    #     rs_vals = np.arange(min_rs, max_rs)
    #     for rs1 in rs_vals:
    #       rs2_start = max(min_rs, rs1-max_delta)
    #       rs2_end = min(max_rs, rs1+max_delta)
    #       for rs2 in np.arange(rs2_start, rs2_end+1):
    #         print(["processing row sums equal to", rs1, rs2])
    #         dc = self.compute_distance_counts(rs1, rs2, mc_samples)
    #         distance_m.append(dc)
    #         rs1_vals = rs1_vals + np.repeat(rs1, len(dc)).tolist()
    #         rs2_vals = rs2_vals + np.repeat(rs2, len(dc)).tolist()
     
    #     tb = pd.concat(distance_m)
    #     tb.insert(0, "rs2", rs2_vals)
    #     tb.insert(0, "rs1", rs1_vals)
      
    #     tb.to_csv(self.cutoffs_file, index=False, sep=",")
        
    # def read_cutoff_file(self):
    #     d = pd.read_csv(self.cutoffs_file, sep=",")
    #     return d
           