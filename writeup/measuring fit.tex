\documentclass{article}

\usepackage{amssymb, amsmath, amsthm, verbatim}

\newcommand{\CC}{\mathcal{C}}

\begin{document}

	We start with a binary matrix $A$ with $N$ columns (cell types) and apply the Louvain algorithm to cluster rows.  Let $X^{(r)}$ be the $L_r \times N$ submatrix of $A$ formed by considering rows in the $r$th cluster generated by the Louvain algorithm.  We next cluster the columns of the  $X^{(r)}$, collectively.  Let $K$ be the number of clusters, $\CC_k$ be the columns in cluster $k$, and $N_k$ be the number of columns in $\CC_k$ for $k=1,2,\dots,K$.  
	
\section{Variance Decomposition}.

	The variance of the $X^{(r)}$ can be decomposed into within column variance, within (column) cluster variance, and across (column) cluster variance.   For simplicity, I'll drop the $r$ and consider simply $X$ with dimensions $L \times C$.  The full variance can be computed by summing over $r$.  Let $N_k$ be the 
	
\begin{align}
\text{Var}(X) &= \frac{1}{LN} \sum_{j=1}^N \sum_{i=1}^L \left(X_{ij} - \bar{x} \right)^2
\\ \notag
	& = \sum_{k=1}^K \frac{N_k}{N} \left[\frac{1}{N_k L} \sum_{j \in \CC_k} \sum_{i=1}^L \left(X_{ij} - \bar{x} \right)^2 \right]
\\ \notag
	& = \sum_{k=1}^K \frac{N_k}{N} \left[\frac{1}{N_k L} \sum_{j \in \CC_k} \sum_{i=1}^L 
	\left((X_{ij} - \bar{X}_{\cdot j}) + (\bar{X}_{\cdot j} - \bar{X}_{\CC_k}) + (\bar{X}_{\CC_k} - \bar{x}) \right)^2 \right]
\\ \notag 
& = \sum_{k=1}^K \frac{N_k}{N} \left[\frac{1}{N_k L} \sum_{j \in \CC_k} \sum_{i=1}^L 
	\left(X_{ij} - \bar{X}_{\cdot j}\right)^2 + \left( \bar{X}_{\cdot j} - \bar{X}_{\CC_k} \right)^2 + \left(\bar{X}_{\CC_k} - \bar{x}) \right)^2 \right],
\end{align}
where $\bar{X}_{\cdot j}$ is the mean of column $j$ and $\bar{X}_{\CC_k}$ is the mean of the submatrix of X formed by the columns in the $k$th cluster.  Since $X$ is binary, the means are just the frequency of $1$'s.  Letting $p_j$ the the frequency of $1$'s in the $j$th column, $p^{(k)}$ be the frequency of $1$'s in the $k$th cluster, and $p$ be the frequency of $1$'s in $X$,  some algebra gives,
\begin{align}
\text{Var}(X) = \frac{1}{LN} \sum_{j=1}^N \sum_{i=1}^L \left(X_{ij} -p_j \right)^2
+ \frac{1}{N} \sum_{k=1}^K \sum_{j \in \CC_k} \left( p_j - p^{(k)} \right)^2 
+ \sum_{k=1}^K \frac{N_k}{N} \left( p^{(k)} - p \right)^2.
\end{align}
The terms on the right of the equality represent within column variance, within cluster variance, and across cluster variance respectively.

\section{Measuring Cluster Fit}
	
	Clustering columns (cell types) cannot alter the within column variance, so our clustering just moves variance between within cluster variance and across cluster variance.  We want to minimize within cluster variance.  The cross cluster variance represents the variance \textit{captured or explained} by the clustering.
	
\begin{equation}
\text{Fraction of Variance Captured} = \frac{\text{Cross Cluster Variance}}{\textit{Total Variance}}
\end{equation}

	We clustered the columns by minimizing the sum of squared residuals (SSE), where the residual was the difference between $X_ij$ and the mediod of the cluster containing column $j$.   In effect, we computed,
\begin{equation}
\text{SSE} =  \sum_{k=1}^K \sum_{j \in \CC_k} \sum_{i=1}^L  \left( X_{ij} - p^{(k)} \right)^2,
\end{equation}
which is proportional the the sum of the within column and within cluster variances.  Since within column variances aren't changed by column clustering, minimizing SSE is equivalent to minimizing within cluster variance or maximizing cross cluster variance.

	Previously, we discussed using the fraction of \textit{energy} captured as a measure of fit,
\begin{equation}
\text{Fraction of Energy Captured} =  1 - \frac{\text{SSE}}{\sum_{ij} X_ij^2},
\end{equation}
But this is no good.
\begin{align}
& \text{Fraction of Energy Captured} 
 = 1 - \frac{\frac{1}{NL} \text{SSE}}{V(X)} \left(\frac{V(X)}{\frac{1}{NL} \sum_{ij} X_ij^2} \right)
\\ \notag &
= 1 - \frac{\text{Within Cluster Varaince + WIthin Column Variance}}{\textit{Total Variance}}
\left(\frac{V(X)}{\frac{1}{NL} \sum_{ij} X_ij^2} \right)
\\ \notag &
= 1 - \left(1 - \text{Fraction of Variance Captured}\right) 
\left(\frac{V(X)}{\frac{1}{NL} \sum_{ij} X_ij^2} \right).
\end{align}
Since $V(X)$ is invariant to flipping $1$'s to $0$'s and vice versa, the extra factor on the far right shows that we will \text{miss} less energy in cases when the fraction of $1$'s is high.  This doesn't seem to be an attribute we want in measuring fit.

\section{Determining Optimal Fit}

	We capture more variance as we allow for more column clusters.  I can think of two ways to determine the optimal number of column clusters.
\begin{enumerate}
\item Look for a hinge in within cluster variance as $K$ increases.    
\item Overfitting will occur when two column clusters exhibit essentially the same behaviour, meaning that their $p^{(k)}$ will be the same over the $X^{(r)}$, i.e. the different row clusters.  This amounts to balancing cluster coherence (low within cluster variance) against cluster separation.
\end{enumerate}

	Column clustering will not reduce the within column variance, which I find to be about $60\%$ of the variance.  Only further clustering of the rows can reduce variance.   Here too, the question becomes how much clustering is too much.  In this case, different row clusters should reflect different cluster means, i.e. the $p^{(k)}$ as vectors, over $k=1,2,\dots,K$, should differ across the $X^{(r)}$.  

\end{document}
